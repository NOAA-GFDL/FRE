#
# $Id: fre.properties,v 1.1.2.1 2014/09/18 15:16:13 sdu Exp $
# ------------------------------------------------------------------------------
# FMS/FRE Project: Site Properties File - GFDL Version for Workstations
# ------------------------------------------------------------------------------
# Copyright (C) NOAA Geophysical Fluid Dynamics Laboratory, 2000-2012
# Designed and written by V. Balaji, Amy Langenhorst and Aleksey Yakovlev
#

#k1p use multiple copies to test hardware
FRE.option.dual.default=0
FRE.option.free.default=1
#k1p automaticall transfer data back to gfdl
FRE.option.transfer.default=1

#Customize the root location of the FRE package for input data, FMS source code
#FRE.distribution.home=/home/user/fre/bronx-public
FRE.distribution.home=/ccs/proj/cli061/local/opt/fre-commands

#Allows environment variable expansion in the runscript template 
#$HOME and $ARCHIVE are available
#FRE.expand.HOME=/home/$USER
FRE.expand.HOME=/autofs/na3_home/$USER

#Customize the default directory locations for your site
FRE.directory.stem.default=$(suite)
FRE.directory.root.default=$HOME/$(stem)
FRE.directory.src.default=$(rootDir)/$(name)/src
FRE.directory.exec.default=$(rootDir)/$(name)/$(platform)-$(target)/exec
FRE.directory.scripts.default=$(rootDir)/$(name)/$(platform)-$(target)/scripts
FRE.directory.state.default=$(rootDir)/$(name)/$(platform)-$(target)/state
FRE.directory.work.default=/lustre/atlas/proj-shared/cli061/$USER/work/$FRE_JOBID
FRE.directory.ptmp.default=/lustre/atlas/proj-shared/cli061/$USER/ptmp
FRE.directory.stmp.default=/lustre/atlas/proj-shared/cli061/$USER/stmp
FRE.directory.stdout.default=$(rootDir)/$(name)/$(platform)-$(target)/stdout
FRE.directory.stdoutTmp.default=$(stdoutDir)
FRE.directory.archive.default=/lustre/atlas/proj-shared/cli061/$USER/$(stem)/$(name)/$(platform)-$(target)
FRE.directory.postProcess.default=$(archiveDir)/pp
FRE.directory.analysis.default=$(archiveDir)/analysis

#Allows separation of the run stdout dir (stdoutTmp) from the final stdoutdir (stdout)
FRE.directory.stdoutTmp.final=1

#Customize the allowed filesystems / root locations
#FRE.directory.scratch.roots=$(PROJWORK)
FRE.directory.scratch.roots=/lustre/atlas/proj-shared

#FRE.directory.roots.default=$HOME;$(PROJWORK)
FRE.directory.roots.default=$HOME;/lustre/atlas/proj-shared;/lustre/atlas/scratch/$USER
FRE.directory.root.roots=$(FRE.directory.roots.default)
FRE.directory.src.roots=$(FRE.directory.roots.default)
FRE.directory.exec.roots=$(FRE.directory.roots.default)
FRE.directory.scripts.roots=$(FRE.directory.roots.default)
FRE.directory.stdout.roots=$(FRE.directory.roots.default)
FRE.directory.stdoutTmp.roots=$(FRE.directory.roots.default)
FRE.directory.state.roots=$(FRE.directory.roots.default)
FRE.directory.work.roots=$(FRE.directory.roots.default)
FRE.directory.ptmp.roots=$(FRE.directory.roots.default)
FRE.directory.stmp.roots=$(FRE.directory.roots.default)
FRE.directory.archive.roots=$(FRE.directory.roots.default)
FRE.directory.postProcess.roots=$(FRE.directory.roots.default)
FRE.directory.analysis.roots=$(FRE.directory.roots.default)

#Requires certain directory types to have the experiment name in them
FRE.directory.expNamed=archive;postProcess;analysis

#Customize whether you're using MPI to run your model executables in parallel
#FRE.mpi.runCommand.default=./$executable:t
#FRE.mpi.runCommand.default=/usr/bin/time -p '`which mpirun`' -n $npes ./$executable:t

#Allows separate models to have special run commands, ie for openmp
#FRE.mpi.runCommand.solo.atmos=$(FRE.mpi.runCommand.default)
#FRE.mpi.runCommand.solo.ocean=$(FRE.mpi.runCommand.default)
#FRE.mpi.runCommand.coupled=$(FRE.mpi.runCommand.default)

FRE.mpi.component.enabled=1;1;0
FRE.mpi.component.names=atmos;ocean;wave
FRE.mpi.component.serials=atmos<ocean;atmos<wave
FRE.mpi.component.suffixes=a;o;w

FRE.mpi.atmos.subComponents=ice;land
FRE.mpi.ocean.subComponents=
FRE.mpi.wave.subComponents=

FRE.mpi.runCommand.launcher=/usr/bin/time -p '`which aprun`'
FRE.mpi.runCommand.option.mpiprocs=-n $
FRE.mpi.runCommand.option.nthreads=-d $
FRE.mpi.runCommand.executable=./$executable:t



#Customize with the location of your module software installation
#FRE.tool.modules.home=/usr/local/Modules/3.2.9
FRE.tool.modules.home=/opt/modules/3.2.6.7

FRE.tool.modules.use.remote=/home/fms/local/modulefiles
FRE.tool.modules.version.remote=test

#Allows a different file archiver other than tar (ie, cpio)
FRE.tool.archiver.extension=tar

#Allows FRE to set the default compiler.mk and change it based on the compiler modules loaded
FRE.tool.mkmf.template.mapping=cray.mk{{^\s*module\s+load\s+PrgEnv-cray}};intel.mk{{^\s*module\s+load\s+PrgEnv-intel}};pathscale.mk{{^\s*module\s+load\s+PrgEnv-pathscale}};pgi.mk{{^\s*module\s+load\s+PrgEnv-pgi}};gnu.mk{{^\s*module\s+load\s+PrgEnv-gnu}};NULL
FRE.tool.mkmf.template.default=intel.mk


#Allows FRE to choose the correct netcdf settings -- recommend to use 4 and leave this line as is
#FRE.tool.make.override.netcdf.mapping=4{{^\s*module\s+load\s+(?:netcdf[-/]4|fre-nctools/[4a-z]|fre/[a-z])}};3

FRE.tool.make.override.netcdf.mapping=4{{^\s*module\s+load\s+(?:netcdf[-/]4|fre-nctools/[4a-z]|fre/[a-z])}};3
## TODO 4.2.0

#Allows fremake to compile in parallel with make -j #
FRE.tool.make.jobs.default=4

#Allows FRE to run fremake and frerun on your site
FRE.tool.fremake.enabled=1
FRE.tool.frerun.enabled=1

#Customize the version control system from which fremake should acquire source code
FRE.versioncontrol.enabled=1
FRE.versioncontrol.cvs.root=:ext:cvs@gitlab.gfdl.noaa.gov:/home/fms/cvs
#TODO mirror status?

#Allows sophisticated output data staging; start with online, which means to combine distributed 
#history data and create tar archives of the output in the runscript template directly
#FRE.output.staging=online
#FRE.output.staging.online.transferOn.archive.roots=$(FRE.directory.roots.default)
#FRE.output.staging.online.transferOff.archive.roots=$(FRE.directory.roots.default)
#FRE.output.staging=chained;staged;online
FRE.output.staging=chained;online

FRE.output.staging.staged.transferOn.archive.roots=$(FRE.directory.scratch.roots)
FRE.output.staging.chained.transferOn.archive.roots=$(FRE.directory.scratch.roots)
FRE.output.staging.online.transferOn.archive.roots=$(FRE.directory.scratch.roots)

FRE.output.staging.staged.transferOff.archive.roots=$(FRE.directory.scratch.roots)
FRE.output.staging.chained.transferOff.archive.roots=$(FRE.directory.scratch.roots)
FRE.output.staging.online.transferOff.archive.roots=$(FRE.directory.scratch.roots)

FRE.scheduler.enabled=1
FRE.scheduler.prefix=#PBS
FRE.scheduler.jobId.length=6

FRE.scheduler.submit.command=qsub
#TODO
FRE.scheduler.submit.output.pattern=^(\d+)

# FRE.scheduler.option.reqNodes if non-zero then use # nodes instead of #cores for job size
FRE.scheduler.option.reqNodes=1
FRE.scheduler.option.dependsOn=-W depend=afterok:$
FRE.scheduler.option.join=-j oe
FRE.scheduler.option.mail=-m $
FRE.scheduler.option.name=-N $
FRE.scheduler.option.priority=-p $
FRE.scheduler.option.project=-A $
FRE.scheduler.option.queue=-q $
FRE.scheduler.option.rerun=-r y
FRE.scheduler.option.shell=-S /bin/tcsh
FRE.scheduler.option.size.inputStager=-l nodes=1
#FRE.scheduler.option.size.make=-l nodes=1:ppn=$
FRE.scheduler.option.size.make=-l nodes=1
FRE.scheduler.option.size.run.distributed=0
FRE.scheduler.option.size.run=-l nodes=$
FRE.scheduler.option.startTime=-a $
FRE.scheduler.option.stdout=-o $/
FRE.scheduler.option.stdoutUmask=-W umask=$
FRE.scheduler.option.time=-l walltime=$
FRE.scheduler.option.workDir=-d $

FRE.scheduler.variable.environment=PBS_ENVIRONMENT
FRE.scheduler.variable.environment.value.batch=PBS_BATCH
FRE.scheduler.variable.jobID=PBS_JOBID

#TODO https://www.olcf.ornl.gov/support/system-user-guides/titan-user-guide/#303
# batch killable debug
#FRE.scheduler.queues=batch;urgent;debug;novel;service;bigmem
FRE.scheduler.queues=batch;debug;killable

FRE.scheduler.outputStager.save.queue=dtn
FRE.scheduler.outputStager.save.coreSpec=nodes=1
FRE.scheduler.outputStager.save.runtime.ascii=4:00:00
FRE.scheduler.outputStager.save.runtime.restart=4:00:00
FRE.scheduler.outputStager.save.runtime.history=4:00:00
FRE.scheduler.outputStager.save.retries=4

FRE.scheduler.outputStager.transfer.queue=dtn
FRE.scheduler.outputStager.transfer.coreSpec=nodes=1
FRE.scheduler.outputStager.transfer.runtime.ascii=8:00:00
FRE.scheduler.outputStager.transfer.runtime.restart=8:00:00
FRE.scheduler.outputStager.transfer.runtime.history=8:00:00
FRE.scheduler.outputStager.transfer.retries=4

FRE.scheduler.workDirCleaner.queue=dtn
FRE.scheduler.workDirCleaner.coreSpec=nodes=1
FRE.scheduler.workDirCleaner.runtime=8:00

FRE.scheduler.ppStarter.coreSpec=size=1
FRE.scheduler.ppStarter.combineOff.largeOff.queue=batch
FRE.scheduler.ppStarter.combineOff.largeOff.runtime=12:00:00
FRE.scheduler.ppStarter.combineOff.largeOn.queue=bigvftmp
FRE.scheduler.ppStarter.combineOff.largeOn.runtime=16:00:00
FRE.scheduler.ppStarter.combineOn.largeOff.queue=stage
FRE.scheduler.ppStarter.combineOn.largeOff.runtime=1:00:00
FRE.scheduler.ppStarter.combineOn.largeOn.queue=stage
FRE.scheduler.ppStarter.combineOn.largeOn.runtime=1:00:00
FRE.scheduler.ppStarter.historySize.threshold=8192

FRE.scheduler.make.coresPerJob.inc=1
FRE.scheduler.make.coresPerJob.max=100000
FRE.scheduler.run.coresPerJob.inc=16
FRE.scheduler.run.coresPerJob.max=100000

#FRE.scheduler.runtime.max=8:00:00
FRE.scheduler.runtime.max=02:00:00{{1984}};06:00:00{{4992}};12:00:00{{59984}};24:00:00

FRE.scheduler.dual.delay=3600
FRE.scheduler.dual.priority=-1
